# --- QMIX specific parameters ---

# --- Agent Parameters ---
obs_agent_id: True
rnn_hidden_dim: 64
action_selector: "noisy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 100000
t_max: 10050000 #number of timesteps to train for before interrupting

worker_parameter_sync_frequency: 10 # How many parameter updates should occur before the workers sync. 

# --- REPLAY Parameters ---
use_per: False # use prioritized experience replay
prioritized_buffer_alpha: 0.6
per_eta: 0.9
buffer_size: 5000 # this will be very heavily dependent on your available RAM, as well as episode length limit

# --- RL hyperparameters ---
batch_size: 128 # number of episodes used to backprop and update networks
random_update: False # whether a subset sequence of transitions should be sampled from each episode
recurrent_sequence_length: 96 # how many transitions should be sampled from each episode in the batch, if random_update = True
lr: 0.001 # Learning rate for agents
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 20 # Reduce magnitude of gradients above this L2 norm

#--- R2D2 Stuff ---
# use_burnin implies that we will store hidden states
use_burnin: False
burn_in_step_count: 32 #number of steps to burn hidden state in
value_fn_rescaling: False

#Ablation
num_executors: 4 # capped at total_num_cpu_threads - 4
n_step_return: False
n_step: 8
standardise_rewards: False

#Transfer_learning:
use_transfer: False
models_2_transfer_path: "./results/full_space_succeed_transfer/models/15000"


# --- Stuff ---
reward_clip_max: 5
reward_clip_min: -5

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64


name: "smac_experiment_1"

# --- Test ---
test_interval: 10000
n_test_episodes: 32

# --- Logging options ---
#use_tensorboard: True # Log results to tensorboard
save_models: True # Save the models to disk
save_models_interval: 5000 # Save models after this many trainer steps
local_results_path: "results" # Path for local results
log_every: 20 #log every time this number of trainer steps has elapsed

env_args:
  continuing_episode: False
  difficulty: "3"
  game_version: null
  map_name: "10gen_protoss"
  move_amount: 2
  obs_all_health: True
  obs_instead_of_state: False
  obs_last_action: False
  obs_own_health: True
  obs_pathing_grid: False
  obs_terrain_height: False
  obs_timestep_number: False
  reward_death_value: 10
  reward_defeat: 0
  reward_negative_scale: 0.5
  reward_only_positive: True
  reward_scale: True
  reward_scale_rate: 20
  reward_sparse: False
  reward_win: 200
  replay_dir: ""
  replay_prefix: ""
  conic_fov: False
  use_unit_ranges: True
  min_attack_range: 2
  obs_own_pos: True
  num_fov_actions: 12
  capability_config:
    n_units: 5
    n_enemies: 5
    team_gen:
      dist_type: "weighted_teams"
      unit_types: 
        - "stalker"
        - "zealot"
        - "colossus"
      weights:
        - 0.45
        - 0.45
        - 0.1
      observe: True
    start_positions:
      dist_type: "surrounded_and_reflect"
      p: 0.5
      map_x: 32
      map_y: 32
    
    # enemy_mask:
    #   dist_type: "mask"
    #   mask_probability: 0.5
    #   n_enemies: 5
  state_last_action: True
  state_timestep_number: False
  step_mul: 8
  heuristic_ai: False
  # heuristic_rest: False
  debug: False
  prob_obs_enemy: 1.0
  action_mask: True


